_base_:
  - cfgs/train/examples/lora_conventional.yaml


# ---------------------------------------

exp_dir: 'exps/test'

pt_name: pt-1
dataset_dir: 'train_data/surtr_3'
unet_rank: 8
text_encoder_rank: 4
batch_size: 4

# ---------------------------------------

model:
  pretrained_model_name_or_path: 'models/hcp/realisticV51'
  clip_skip: 1

tokenizer_pt:
  emb_dir: 'models/embs/' #自定义word目录
  replace: False #训练后是否替换原有word
  train: 
    - name: ${pt_name}
      lr: 0.003

lora_unet:
  - lr: 1e-4
    rank: ${unet_rank}
    layers:
      - 're:.*\.attn.?$'
      - 're:.*\.ff$'

lora_text_encoder:
  - lr: 1e-5
    rank: ${text_encoder_rank}
    layers:
      - 're:.*self_attn$'
      - 're:.*mlp$'

data:
  dataset1:
    batch_size: ${batch_size}
    cache_latents: True

    source:
      data_source1:
        img_root: ${dataset_dir}
        prompt_template: 'prompt_tuning_template/object_caption.txt'
        caption_file: ${dataset_dir}  # path to image captions (file_words)

        word_names:
          pt1: ${pt_name}

    # support images with any size, not recommended for anime training
    # bucket:
    #   _target_: hcpdiff.data.bucket.RatioBucket.from_files # aspect ratio bucket
    #   target_area: ${times:512,512}
    #   num_bucket: 5

    # all images must have the same size, such as 512x704
    bucket:
      _target_: hcpdiff.data.bucket.SizeBucket.from_files # aspect ratio bucket
      target_area: ---
      num_bucket: 1

logger:
  - _target_: hcpdiff.loggers.CLILogger
    _partial_: True
    out_path: 'logs/train.log'
    log_step: 20
#  - _target_: hcpdiff.loggers.TBLogger
#    _partial_: True
#    out_path: 'logs/tblog/'
#    log_step: 5
#  - _target_: hcpdiff.loggers.WanDBLogger
#    _partial_: True
#    out_path: null
#    log_step: 5


